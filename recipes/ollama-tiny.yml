name: ollama-llm
description: Ollama LLM inference server with TinyLlama
image: docker://ollama/ollama:latest

resources:
  cpu_cores: 8
  memory_gb: 32
  gpu_count: 1
  disk_gb: 50
  nodes: 1
  ntasks: 1
  ntasks_per_node: 1

ports:
  - container_port: 11434
    host_port: 11434
    protocol: tcp

environment:
  OLLAMA_HOST: "0.0.0.0:11434"
  OLLAMA_MODELS: "/root/.ollama/models"
  # SSL Certificates
  SSL_CERT_FILE: "/etc/ssl/certs/ca-certificates.crt"
  CURL_CA_BUNDLE: "/etc/ssl/certs/ca-certificates.crt"

volumes:
  # Model storage persistenti
  - host_path: /project/scratch/$SLURM_JOB_ACCOUNT/$USER/ollama
    container_path: /root/.ollama
    readonly: false
  
  - host_path: /etc/ssl/certs
    container_path: /etc/ssl/certs
    readonly: true
  
  - host_path: /etc/pki
    container_path: /etc/pki
    readonly: true

healthcheck:
  endpoint: /
  interval_seconds: 10
  timeout_seconds: 5
  retries: 3
  initial_delay: 30

command:
  - /bin/bash
  - -c
  - |
    set -e
    
    echo "========================================="
    echo "Ollama Service Starting"
    echo "========================================="
    echo "Hostname: $(hostname)"
    echo "Date: $(date)"
    echo "========================================="
    
    
    echo "Checking SSL certificates..."
    if [ -f /etc/ssl/certs/ca-certificates.crt ]; then
        echo "✓ CA certificates found"
    else
        echo "⚠ Warning: CA certificates not found"
    fi
    
    # Start Ollama server in background
    echo "Starting Ollama server..."
    ollama serve &
    SERVER_PID=$!
    echo "✓ Server PID: $SERVER_PID"
    
    # Cleanup function
    cleanup() {
        echo ""
        echo "Shutting down Ollama server..."
        kill $SERVER_PID 2>/dev/null || true
        wait $SERVER_PID 2>/dev/null || true
        echo "✓ Server stopped"
    }
    trap cleanup EXIT INT TERM
    
    # Wait for server
    echo "Waiting for server to initialize (15 seconds)..."
    sleep 15
    
    # Pull model
    echo ""
    echo "Downloading TinyLlama model (1.1B parameters)..."
    if ollama pull tinyllama; then
        echo "✓ Model downloaded successfully"
    else
        echo "✗ Model download failed"
        exit 1
    fi
    
    # Display info
    COMPUTE_NODE=$(hostname)
    echo ""
    echo "========================================="
    echo "Ollama Server Ready!"
    echo "========================================="
    echo "API Endpoint: http://${COMPUTE_NODE}:11434"
    echo "Model: tinyllama"
    echo ""
    echo "Test from login node:"
    echo "  curl http://${COMPUTE_NODE}:11434/api/tags"
    echo ""
    echo "Example generation:"
    echo '  curl http://'"${COMPUTE_NODE}"':11434/api/generate \'
    echo '    -d '"'"'{"model":"tinyllama","prompt":"Hello!","stream":false}'"'"
    echo ""
    echo "Job will run until time limit or manual cancellation"
    echo "========================================="
    
    # Keep alive
    wait $SERVER_PID
    EXIT_CODE=$?
    echo ""
    echo "Server exited with code: $EXIT_CODE"
    exit $EXIT_CODE

working_dir: /workspace