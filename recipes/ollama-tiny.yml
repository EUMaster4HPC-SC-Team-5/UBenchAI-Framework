# Ollama LLM inference service for MeluXina
# Compatible with LuxProvide Apptainer setup

name: ollama-llm
description: Ollama LLM inference service with GPU support
image: docker://ollama/ollama:latest

resources:
  cpu_cores: 4
  memory_gb: 16
  gpu_count: 1
  # gpu_type: null  # Don't specify - let MeluXina auto-detect
  disk_gb: 50
  nodes: 1
  ntasks: 1

ports:
  - container_port: 11434
    host_port: 11434
    protocol: tcp

environment:
  OLLAMA_HOST: "0.0.0.0:11434"
  OLLAMA_ORIGINS: "*"
  OLLAMA_NUM_PARALLEL: "4"

volumes:
  # Project scratch directory for persistent model storage
  - host_path: /project/scratch/$SLURM_JOB_ACCOUNT/$USER/ollama # put a team0 folder you have the permission right to write in
    container_path: /root/.ollama
    readonly: false

healthcheck:
  endpoint: /api/tags
  interval_seconds: 15
  timeout_seconds: 10
  retries: 3
  initial_delay: 30

# Command executed INSIDE the Apptainer container
command:
  - /bin/bash
  - -c
  - |
    set -e
    
    echo "========================================="
    echo "Ollama Service Starting"
    echo "========================================="
    echo "Hostname: $(hostname)"
    echo "Date: $(date)"
    echo "========================================="
    
    # Start Ollama server in background
    echo "Starting Ollama server..."
    ollama serve > /tmp/ollama-server.log 2>&1 &
    SERVER_PID=$!
    echo "✓ Server PID: $SERVER_PID"
    
    # Cleanup function
    cleanup() {
        echo ""
        echo "Shutting down Ollama server..."
        kill $SERVER_PID 2>/dev/null || true
        wait $SERVER_PID 2>/dev/null || true
        echo "✓ Server stopped"
    }
    trap cleanup EXIT INT TERM
    
    echo "Waiting for server to initialize (15 seconds)..."
    sleep 15

    if ! kill -0 $SERVER_PID 2>/dev/null; then
        echo "✗ Server process died during startup"
        echo "=== Server Logs ==="
        cat /tmp/ollama-server.log
        exit 1
    fi
    
    # Download TinyLlama model
    echo ""
    echo "Downloading TinyLlama model (1.1B parameters)..."
    if ollama pull tinyllama 2>&1; then
        echo "✓ Model downloaded successfully"
    else
        echo "✗ Model download failed"
        exit 1
    fi
    
    # List available models
    echo ""
    echo "Available models:"
    ollama list
    
    # Display connection information
    COMPUTE_NODE=$(hostname)
    echo ""
    echo "========================================="
    echo "Ollama Server Ready!"
    echo "========================================="
    echo "Endpoint: http://${COMPUTE_NODE}:11434"
    echo ""
    echo "Test from login node or other jobs:"
    echo "  curl http://${COMPUTE_NODE}:11434/api/tags"
    echo ""
    echo "Example inference request:"
    echo "  curl http://${COMPUTE_NODE}:11434/api/generate -d '{"
    echo "    \"model\": \"tinyllama\","
    echo "    \"prompt\": \"What is artificial intelligence?\","
    echo "    \"stream\": false"
    echo "  }'"
    echo ""
    echo "Server logs: /tmp/ollama-server.log"
    echo "Job will run until time limit or manual cancellation"
    echo "========================================="
    
    # Keep container alive - wait for server process
    wait $SERVER_PID
    EXIT_CODE=$?
    echo ""
    echo "Server exited with code: $EXIT_CODE"
    exit $EXIT_CODE

working_dir: /root
