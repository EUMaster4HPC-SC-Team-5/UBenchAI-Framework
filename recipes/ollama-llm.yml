# Sample service recipe for Ollama LLM inference service

name: ollama-llm
description: Ollama LLM inference service with GPU support
image: docker://ollama/ollama:latest

resources:
  cpu_cores: 4
  memory_gb: 16
  gpu_count: 1
  gpu_type: nvidia
  disk_gb: 50
  nodes: 1 # Use 4 compute nodes
  ntasks: 1 # Total 16 tasks across all nodes

ports:
  - container_port: 11434
    host_port: 11434
    protocol: tcp

environment:
  OLLAMA_HOST: "0.0.0.0"
  OLLAMA_ORIGINS: "*"
  OLLAMA_NUM_PARALLEL: "4"

volumes:
  - host_path: /project/scratch/$SLURM_JOB_ACCOUNT/team-5/ollama/models
    container_path: /root/.ollama
    readonly: false

healthcheck:
  endpoint: /api/tags
  interval_seconds: 15
  timeout_seconds: 10
  retries: 3
  initial_delay: 30

command:
  - /bin/bash
  - -c
  - |
    echo "========================================="
    echo "TinyLlama Quick Test"
    echo "========================================="

    # Start Ollama server in background
    ollama serve > /tmp/ollama.log 2>&1 &
    SERVER_PID=$!
    echo "Started Ollama server (PID: $SERVER_PID)"

    # Wait for server to be ready (max 60 seconds)
    echo "Waiting for server to be ready..."
    for i in {1..30}; do
      if curl -s http://localhost:11434/api/tags >/dev/null 2>&1; then
        echo "✓ Server is ready!"
        break
      fi
      sleep 2
    done

    # Pull TinyLlama (1.1B parameters - small and fast)
    echo ""
    echo "Pulling TinyLlama model..."
    ollama pull tinyllama
    echo "✓ Model pulled successfully"

    # Run test prompts
    echo ""
    echo "========================================="
    echo "Running test prompts..."
    echo "========================================="

    echo ""
    echo "Test 1: Simple question"
    ollama run tinyllama "What is 2+2? Answer in one short sentence." --verbose

    echo ""
    echo "Test 2: Code generation"
    ollama run tinyllama "Write a Python function to add two numbers. Just show the code." --verbose

    echo ""
    echo "Test 3: Creative writing"
    ollama run tinyllama "Write a haiku about programming." --verbose

    # Show model info
    echo ""
    echo "========================================="
    echo "Available models:"
    ollama list

    echo ""
    echo "Model information:"
    ollama show tinyllama

    # Cleanup
    echo ""
    echo "========================================="
    echo "Test completed successfully!"
    echo "Stopping server..."
    kill $SERVER_PID
    wait $SERVER_PID 2>/dev/null
    echo "✓ Server stopped"
    echo "========================================="

working_dir: /root
