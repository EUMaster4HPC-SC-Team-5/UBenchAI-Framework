# vLLM Multi-Node LLM inference service for MeluXina
# Uses tensor parallelism across multiple GPUs/nodes for large models
# Example: Llama-2-70B or Mixtral-8x7B across 4 GPUs

name: vllm-multinode
description: vLLM multi-node inference with tensor parallelism for large models
image: docker://vllm/vllm-openai:v0.6.3.post1

resources:
  cpu_cores: 8
  memory_gb: 128
  gpu_count: 4  # 4 GPUs total (can span multiple nodes)
  disk_gb: 200
  nodes: 2  # Use 2 nodes with 2 GPUs each
  ntasks: 2  # 1 task per node
  ntasks_per_node: 1

ports:
  - container_port: 8000
    host_port: 8000
    protocol: tcp

environment:
  VLLM_HOST: "0.0.0.0"
  VLLM_PORT: "8000"
  HF_HOME: "/root/.cache/huggingface"
  # Multi-node communication
  NCCL_DEBUG: "INFO"
  NCCL_IB_DISABLE: "0"
  NCCL_SOCKET_IFNAME: "ib0"
  # Tensor parallelism configuration
  VLLM_WORKER_MULTIPROC_METHOD: "spawn"

volumes:
  # Project scratch directory for persistent model storage
  - host_path: /project/scratch/$SLURM_JOB_ACCOUNT/team-5/vllm
    container_path: /root/.cache/huggingface
    readonly: false

healthcheck:
  endpoint: /health
  interval_seconds: 20
  timeout_seconds: 15
  retries: 3
  initial_delay: 120  # Multi-node setup takes longer

# Command executed INSIDE the Apptainer container
command:
  - /bin/bash
  - -c
  - |
    set -e
    
    echo "========================================="
    echo "vLLM Multi-Node Service Starting"
    echo "========================================="
    echo "Job ID: $SLURM_JOB_ID"
    echo "Hostname: $(hostname)"
    echo "Date: $(date)"
    echo "Nodes: $SLURM_JOB_NUM_NODES"
    echo "Node List: $SLURM_JOB_NODELIST"
    echo "GPUs per Node: 2"
    echo "Total GPUs: 4"
    echo "vLLM Version: v0.6.3.post1"
    echo "========================================="
    
    # Check GPU availability on this node
    echo ""
    echo "Checking GPU availability on $(hostname)..."
    if command -v nvidia-smi >/dev/null 2>&1; then
        nvidia-smi --query-gpu=index,name,memory.total --format=csv,noheader
        GPU_COUNT=$(nvidia-smi --query-gpu=name --format=csv,noheader | wc -l)
        echo "✓ Found $GPU_COUNT GPU(s) on this node"
    else
        echo "Warning: nvidia-smi not found"
    fi
    
    # Get the master node address (first node in allocation)
    MASTER_NODE=$(scontrol show hostnames $SLURM_JOB_NODELIST | head -n 1)
    echo ""
    echo "Master Node: $MASTER_NODE"
    echo "Current Node: $(hostname)"
    
    # Determine if this is the master node
    if [ "$(hostname)" = "$MASTER_NODE" ]; then
        IS_MASTER=true
        echo "✓ This is the MASTER node"
    else
        IS_MASTER=false
        echo "✓ This is a WORKER node"
    fi
    
    # Start vLLM server with tensor parallelism
    # Only the master node exposes the API endpoint
    echo ""
    echo "Starting vLLM server with tensor parallelism..."
    echo "Model: mistralai/Mistral-7B-Instruct-v0.2"
    echo "Tensor Parallel Size: 4 (across 2 nodes)"
    
    if [ "$IS_MASTER" = true ]; then
        echo ""
        echo "========================================="
        echo "MASTER NODE: Starting API Server"
        echo "========================================="
        
        # Master node runs the API server with tensor parallelism
        python3 -m vllm.entrypoints.openai.api_server \
            --model mistralai/Mistral-7B-Instruct-v0.2 \
            --host 0.0.0.0 \
            --port 8000 \
            --tensor-parallel-size 4 \
            --gpu-memory-utilization 0.9 \
            --max-model-len 8192 \
            --dtype auto \
            --trust-remote-code \
            --disable-log-requests &
        
        SERVER_PID=$!
        echo "✓ Server PID: $SERVER_PID"
        
        # Cleanup function
        cleanup() {
            echo ""
            echo "Shutting down vLLM server..."
            kill $SERVER_PID 2>/dev/null || true
            wait $SERVER_PID 2>/dev/null || true
            echo "✓ Server stopped"
        }
        trap cleanup EXIT INT TERM
        
        # Wait for server initialization (longer for multi-node)
        echo "Waiting for server to initialize (120 seconds)..."
        sleep 120
        
        if ! kill -0 $SERVER_PID 2>/dev/null; then
            echo "✗ Server process died during startup"
            echo "Check logs for details"
            exit 1
        fi
        
        # Display connection information
        COMPUTE_NODE=$(hostname)
        echo ""
        echo "========================================="
        echo "vLLM Multi-Node Server Ready!"
        echo "========================================="
        echo "OpenAI-compatible API: http://${COMPUTE_NODE}:8000/v1"
        echo "Tensor Parallel Size: 4 GPUs across $SLURM_JOB_NUM_NODES nodes"
        echo ""
        echo "Test from login node:"
        echo "  curl http://${COMPUTE_NODE}:8000/health"
        echo "  curl http://${COMPUTE_NODE}:8000/v1/models"
        echo ""
        echo "Example inference request:"
        echo "  curl http://${COMPUTE_NODE}:8000/v1/completions \\"
        echo "    -H 'Content-Type: application/json' \\"
        echo "    -d '{"
        echo "      \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\","
        echo "      \"prompt\": \"Explain tensor parallelism:\","
        echo "      \"max_tokens\": 200"
        echo "    }'"
        echo ""
        echo "Performance test script:"
        echo "  python3 /workspace/benchmark_multinode.py"
        echo ""
        echo "Job will run until time limit or manual cancellation"
        echo "========================================="
        
        # Keep container alive - wait for server process
        wait $SERVER_PID
        EXIT_CODE=$?
        echo ""
        echo "Server exited with code: $EXIT_CODE"
        exit $EXIT_CODE
        
    else
        echo ""
        echo "========================================="
        echo "WORKER NODE: Waiting for work"
        echo "========================================="
        echo "This node will be utilized by the master"
        echo "for tensor parallel computation."
        echo ""
        echo "Nothing to do here - waiting for job completion..."
        
        # Worker nodes just wait for the job to complete
        # They are utilized by Ray/vLLM automatically
        while true; do
            sleep 60
            if ! squeue -j $SLURM_JOB_ID &>/dev/null; then
                echo "Job completed, exiting worker node"
                break
            fi
        done
    fi

working_dir: /workspace
